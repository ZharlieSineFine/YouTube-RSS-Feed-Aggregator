So what I want to build is an AI news aggregator
where I can take multiple sources. So
for example, YouTube channels and I want
for example blog posts from OpenAI and
Anthropic and I want to scrape those put
them into a database where we have some
kind of structure where we have sources
we have letâ€˜s call them articles and
then what I want to do is I want to run
a daily digest where we're going to take
all of the articles from within that
time frame and we're going to do an LLM
summary around that and then based on
the user insights that we specify in
some kind of
agent system prompt. We can generate a
daily digest which is going to be short
snippets with a link to the original
source. Now from the YouTube channels I
want to I want to be able to create a
list of channels and then we want to get
the latest videos from those channels. I
think we can use the YouTube RSS feed
for that and for for the blog posts we
can just have URLs that we can scrape
for that. Okay. So I want everything
built in a Python back end. I want to
use a Postgress SQL database. I want to
use SQL alchemy in order to define the
database models and then to also create
the tables. I want the project structure
to be an app folder where all the app
logic is in. And then I also want a
docker folder where we first create a
very minimal setup for the Postgress SQL
database. And then that will probably be
starting point. We want to make sure
that later down the line, we can easily
deploy the whole app to render and then
also schedule it every 24 hours to run
the reports, get everything. And then
when we've created the daily digest, I
want to send an email to my personal
inbox with this. So that's that's what I
have in mind. I think that would be
pretty cool to build.




so yeah let's use open AI how
should we send emails I have no idea I
want it to be as easy as possible and
ideally I want viewers of this this
whole build to be easy to follow along.
So, ideally a free surface, but let me
know what do you think is the easiest
way. Blog scraping. I want full I want
the full context of the page. User
insights prompt. Where should the agent
system prompt live? We're going to
create a separate folder called agent
where we'll put all of that in. And the
Docker setup should
include a postcard SQL container. Yeah,
we're not going to connect to an in
external database. We'll do everything
from with inside the project.
Please adjust the plan accordingly.



Can you now create the project structures for me?
So, just lay out the folders already.
You don't have to create the files just yet. And also give me the u add command
with all of the libraries that we're
probably going to need. Don't add them.
add them to the pi project autotoml. I will run it myself.


let's now actually start with building out the YouTube surface where I
want to specify a list of channels and
then for those channels I want to create
a surface where through the YouTube RSS
feed we somehow need I think we need to
figure out the ID or something like
that. You need to check what the input
for the RSS feed is going to be and then
get the latest videos and then also
filter them by time. So every day we can
essentially do a search and see look in
the last 24 hours or so is there a new
video for the channel that we are
creating. Later we can get more specific
about this when we store them into the
database. But let's just first test that
core functionality to see if we can set
that up and then also use the YouTube
transcript API Python library in order
to get the transcript. That's the first
surface that I want you to work on.


look this is the data model the fetch transcript model that we're getting back. How can we just get the text from this?


I want you to migrate this function to the get transcript and I want you to put everything into a nice YouTube. You know
what actually how should we do it? Yeah, I want you to create put it in a scraper and I want you want you to create a YouTube scraper
surface. So put it all nicely into a single class so we can have a nice interface to work with this.


